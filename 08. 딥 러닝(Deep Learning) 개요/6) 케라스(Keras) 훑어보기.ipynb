{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6) 케라스(Keras) 훑어보기.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNt4Z2w0FhWJJKo+8U0C+Ir"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"01ax87hX0Yh8"},"source":["# 6) 케라스(Keras) 훑어보기\n"]},{"cell_type":"markdown","metadata":{"id":"p-xlmsqX0cTI"},"source":["## 1. 전처리(Preprocessing)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCD0el8ktIx4","executionInfo":{"status":"ok","timestamp":1607069848331,"user_tz":-540,"elapsed":724,"user":{"displayName":"John Jeong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihzvyH0ALp6qi818Vtw0vjvcLT3hrB3_0tC8tV=s64","userId":"07771705339356748331"}},"outputId":"a273ce3b-90f2-40f9-ac57-e9f2f5cef556"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","t  = Tokenizer()\n","fit_text = \"The earth is an awesome place live\"\n","t.fit_on_texts([fit_text])\n","\n","test_text = \"The earth is an great place live\"\n","sequences = t.texts_to_sequences([test_text])[0]\n","\n","print(\"sequences : \",sequences) # great는 단어 집합(vocabulary)에 없으므로 출력되지 않는다.\n","print(\"word_index : \",t.word_index) # 단어 집합(vocabulary) 출력"],"execution_count":5,"outputs":[{"output_type":"stream","text":["sequences :  [1, 2, 3, 4, 6, 7]\n","word_index :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvGj0WVF0uX_","executionInfo":{"status":"ok","timestamp":1607070030503,"user_tz":-540,"elapsed":788,"user":{"displayName":"John Jeong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihzvyH0ALp6qi818Vtw0vjvcLT3hrB3_0tC8tV=s64","userId":"07771705339356748331"}},"outputId":"10c68423-4acd-4448-bd92-199cb819c84e"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre')\n","# 전처리가 끝나서 각 단어에 대한 정수 인코딩이 끝났다고 가정하고, 3개의 데이터를 입력으로 합니다.\n","\n","# 첫번째 인자 = 패딩을 진행할 데이터\n","# maxlen = 모든 데이터에 대해서 정규화 할 길이\n","# padding = 'pre'를 선택하면 앞에 0을 채우고 'post'를 선택하면 뒤에 0을 채움."],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 2, 3],\n","       [4, 5, 6],\n","       [0, 7, 8]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xw60FnOS1pcY","executionInfo":{"status":"ok","timestamp":1607070188812,"user_tz":-540,"elapsed":717,"user":{"displayName":"John Jeong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihzvyH0ALp6qi818Vtw0vjvcLT3hrB3_0tC8tV=s64","userId":"07771705339356748331"}},"outputId":"49ad721b-af69-4be8-d87e-4170e0be0524"},"source":["pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=4, padding='post')"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 2, 3, 0],\n","       [3, 4, 5, 6],\n","       [7, 8, 0, 0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"0i7pJjYe2_pF"},"source":["## 2. 워드 임베딩(Word Embedding)\n"]},{"cell_type":"code","metadata":{"id":"Xq6fO-3C2HeF"},"source":["# 문장 토큰화와 단어 토큰화\n","text=[['Hope', 'to', 'see', 'you', 'soon'],['Nice', 'to', 'see', 'you', 'again']]\n","\n","# 각 단어에 대한 정수 인코딩\n","text=[[0, 1, 2, 3, 4],[5, 1, 2, 3, 6]]\n","\n","# 위 데이터가 아래의 임베딩 층의 입력이 된다.\n","Embedding(7, 2, input_length=5)\n","# 7은 단어의 개수. 즉, 단어 집합(vocabulary)의 크기이다.\n","# 2는 임베딩한 후의 벡터의 크기이다.\n","# 5는 각 입력 시퀀스의 길이. 즉, input_length이다.\n","\n","# 각 정수는 아래의 테이블의 인덱스로 사용되며 Embeddig()은 각 단어에 대해 임베딩 벡터를 리턴한다.\n","+------------+------------+\n","|   index    | embedding  |\n","+------------+------------+\n","|     0      | [1.2, 3.1] |\n","|     1      | [0.1, 4.2] |\n","|     2      | [1.0, 3.1] |\n","|     3      | [0.3, 2.1] |\n","|     4      | [2.2, 1.4] |\n","|     5      | [0.7, 1.7] |\n","|     6      | [4.1, 2.0] |\n","+------------+------------+\n","# 위의 표는 임베딩 벡터가 된 결과를 예로서 정리한 것이고 Embedding()의 출력인 3D 텐서를 보여주는 것이 아님."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uc1nemf14Ljm"},"source":["## 3. 모델링(Modeling)\n"]},{"cell_type":"code","metadata":{"id":"S03QEkzW3ubG"},"source":["from tensorflow.keras.models import Sequential\n","model = Sequential()\n","model.add(...) # 층 추가\n","model.add(...) # 층 추가\n","model.add(...) # 층 추가"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJIHzXt14ZjH"},"source":["from tensorflow.keras.models import Sequential\n","model = Sequential()\n","model.add(Embedding(vocabulary, output_dim, input_length))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqJZC3IV4bSm"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","model = Sequential()\n","model.add(Dense(1, input_dim=3, activation='relu'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AaD864SG6Apg"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","model = Sequential()\n","model.add(Dense(8, input_dim=4, activation='relu'))\n","model.add(Dense(1, activation='sigmoid')) # 출력층"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsIi-vUn6f-D"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfpKqvl_6ptN"},"source":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 8)                 40        \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 9         \n","=================================================================\n","Total params: 49\n","Trainable params: 49\n","Non-trainable params: 0\n","_________________________________________________________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CZW2F7OL7C4v"},"source":["## 4. 컴파일(Compile)과 훈련(Training)\n"]},{"cell_type":"code","metadata":{"id":"xhyeoJry6rqb"},"source":["# 이 코드는 뒤의 텍스트 분류 챕터의 스팸 메일 분류하기 실습 코드를 갖고온 것임.\n","from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n","from tensorflow.keras.models import Sequential\n","max_features = 10000\n","\n","model = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(SimpleRNN(32)) #RNN에 대한 설명은 뒤의 챕터에서 합니다.\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZND9GuDw8GDW"},"source":["# 위의 compile() 코드의 연장선상인 코드\n","model.fit(X_train, y_train, epochs=10, batch_size=32)\n","\n","# 첫번째 인자 = 훈련 데이터에 해당됩니다.\n","# 두번째 인자 = 지도 학습에서 레이블 데이터에 해당됩니다.\n","# epochs = 에포크. 에포크 1은 전체 데이터를 한 차례 훑고 지나갔음을 의미함. 정수값 기재 필요. 총 훈련 횟수를 정의합니다.\n","# batch_size = 배치 크기. 기본값은 32. 미니 배치 경사 하강법을 사용하고 싶지 않을 경우에는 batch_size=None을 기재합니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2aprFhn7Hcv"},"source":["model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-UCvEeD68dqK"},"source":["validation_data(x_val, y_val) = 검증 데이터(validation data)를 사용합니다. 검증 데이터를 사용하면 각 에포크마다 검증 데이터의 정확도도 함께 출력되는데, 이 정확도는 훈련이 잘 되고 있는지를 보여줄 뿐이며 실제로 모델이 검증 데이터를 학습하지는 않습니다. 검증 데이터의 loss가 낮아지다가 높아지기 시작하면 이는 과적합(overfitting)의 신호입니다.\n","\n","validation_split= validation_data 대신 사용할 수 있습니다. 검증 데이터를 사용하는 것은 동일하지만, 별도로 존재하는 검증 데이터를 주는 것이 아니라 X_train과 y_train에서 일정 비율을 분리하여 이를 검증 데이터로 사용합니다. 역시나 훈련 자체에는 반영되지 않고 훈련 과정을 지켜보기 위한 용도로 사용됩니다. 아래는 validation_data 대신에 validation_split을 사용했을 경우를 보여줍니다."]},{"cell_type":"code","metadata":{"id":"MLIRTBaR8x_O"},"source":["# 훈련 데이터의 20%를 검증 데이터로 사용.\n","model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_split=0.2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFqra5x58yeh"},"source":["verbose = 학습 중 출력되는 문구를 설정합니다.\n","- 0 : 아무 것도 출력하지 않습니다.\n","- 1 : 훈련의 진행도를 보여주는 진행 막대를 보여줍니다.\n","- 2 : 미니 배치마다 손실 정보를 출력합니다."]},{"cell_type":"markdown","metadata":{"id":"bEqsWsAJ89sp"},"source":["## 5. 평가(Evaluation)와 예측(Prediction)\n"]},{"cell_type":"code","metadata":{"id":"HGeVjQ9r80sW"},"source":["# 위의 fit() 코드의 연장선상인 코드\n","model.evaluate(X_test, y_test, batch_size=32) # evaluate() : 테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가합니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73khj5EN9Bdm"},"source":["# 위의 fit() 코드의 연장선상인 코드\n","model.predict(X_input, batch_size=32) # predict() : 임의의 입력에 대한 모델의 출력값을 확인합니다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyyTxZlm9Xmk"},"source":["## 6. 모델의 저장(Save)과 로드(Load)\n"]},{"cell_type":"code","metadata":{"id":"Nea8Xvnt9G12"},"source":["model.save(\"model_name.h5\") # save() : 인공 신경망 모델을 hdf5 파일에 저장합니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WU0-Rc5M9bmU"},"source":["from tensorflow.keras.models import load_model\n","model = load_model(\"model_name.h5\") # load_model() : 저장해둔 모델을 불러옵니다."],"execution_count":null,"outputs":[]}]}