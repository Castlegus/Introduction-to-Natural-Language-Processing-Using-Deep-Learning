{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02) - 04) 정제,정규화,어간추출,표제어추출,불용어.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTAMwxQvJK92"
      },
      "source": [
        "# 02) 정제(Cleaning) and 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4-hic2tGgnQ",
        "outputId": "7d70d4cb-2a4b-4cb5-a9e4-de3144567ab0"
      },
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB9QCpH-JPOa"
      },
      "source": [
        "# 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXSyk55UKDcc",
        "outputId": "09106925-1ac3-4b87-eb11-d0069390c49a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeePUOovIMV3",
        "outputId": "e6bdd5ea-ed44-4073-b87f-a198e0182157"
      },
      "source": [
        "# 표제어 추출\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([n.lemmatize(w) for w in words])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b44UjTHbJ0ly",
        "outputId": "c4e44bea-d488-4279-af4a-7457ef4b9e9d"
      },
      "source": [
        "# WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있다.\n",
        "n.lemmatize('dies', 'v')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'die'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FaXXOJ0wKYAA",
        "outputId": "0e4f9ffe-0870-49ad-fa5f-502b21ae79fc"
      },
      "source": [
        "n.lemmatize('watched', 'v')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'watch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KTW7oguIKZz4",
        "outputId": "5a8ac568-301f-4fc3-b409-b9bdd26fecfb"
      },
      "source": [
        "n.lemmatize('has', 'v')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWdk8565K1bu",
        "outputId": "e5eadf45-939d-49e4-89c7-629f789f8fa2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WqfTj-ZKbbK",
        "outputId": "39f6c405-aa3c-497e-de26-f9ea5974300d"
      },
      "source": [
        "# 어간 추출\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "s = PorterStemmer()\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLpVc5vmKzS6",
        "outputId": "4e573f5f-17c8-42a3-815f-b695cf212c92"
      },
      "source": [
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBUIcuA8LB4I",
        "outputId": "3aa2611d-5a6e-4b72-ba52-c69188292260"
      },
      "source": [
        "words=['formalize', 'allowance', 'electricical']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG8XROoyLSVw",
        "outputId": "7a4aca11-de21-440f-efdc-e1a09287fe46"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "s=PorterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi3tZ7xMLbOa",
        "outputId": "047716b7-3511-4402-ddfb-d9a1e4c39a8b"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "l=LancasterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([l.stem(w) for w in words])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x89pYB8FMVgt"
      },
      "source": [
        "# 04) 불용어(Stopword)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PbFKv3aNOnO",
        "outputId": "86d1bf03-a3bc-475b-d760-4c5866ec6e3d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUOJCUeDLg-4",
        "outputId": "166be17a-3494-41c3-c196-a56cb3cc2e1b"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "stopwords.words('english')[:10]  "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5zH9P-TNL9R",
        "outputId": "437bffe3-cb65-40e2-8e1c-4f3d56967052"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "print(stop_words)\n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "print(word_tokens) \n",
        "\n",
        "result = []\n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "\n",
        "print(result) "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"wasn't\", 'just', \"doesn't\", 'doing', 'it', 'herself', 'the', 'each', 'ourselves', 'do', 's', 'him', 'who', 'not', 'now', 'most', 't', 'into', 'while', 'itself', 'such', \"should've\", \"shouldn't\", 'wasn', 'nor', 'more', 'd', 'he', 'only', 'this', 'because', 'no', 'shouldn', 'didn', \"wouldn't\", 'had', 'me', 'theirs', 'isn', 'mustn', \"shan't\", 'which', 'were', 'she', 'being', 'whom', 'before', 'if', 'an', 'aren', 'below', 'very', 'was', 'be', 'yourself', 'to', 'against', 'yours', 'under', 'through', 'ain', 'm', 'couldn', 'why', \"she's\", 'don', 'them', \"hasn't\", 'hers', 'o', 'myself', 'won', 'am', 'out', 'than', 'all', 'up', 'what', 'once', 'own', 'or', 'there', 'when', 'll', 'some', 'from', \"didn't\", 'and', 'his', 'until', 'needn', 'but', 'about', 'at', 'down', \"that'll\", 'should', 'you', 'i', 'having', 'haven', 'ours', 'few', 'further', 'shan', 'by', \"isn't\", \"it's\", \"you'll\", \"you've\", 'been', 'over', \"needn't\", 'are', 'doesn', \"don't\", \"aren't\", 'my', 'how', 'y', 'after', 'our', 'its', 'yourselves', 'her', 'a', 'mightn', 'has', 'in', 'will', 'any', 're', 'hadn', 'off', \"mustn't\", \"couldn't\", \"hadn't\", 'these', \"won't\", 'your', 'between', 'ma', 'we', 'themselves', 'on', \"you're\", 'so', 'does', 'wouldn', 'can', 'they', 'their', 've', 'weren', 'then', 'above', 'that', 'here', 'other', 'during', 'for', 'again', \"mightn't\", 'as', \"haven't\", 'of', 'both', 'too', 'is', 'hasn', 'have', 'did', 'where', 'with', \"you'd\", 'those', 'himself', \"weren't\", 'same'}\n",
            "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnM5s6LHNkHv",
        "outputId": "ddf2646b-ca16-4523-9ebf-02444a3a7af1"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
        "stop_words=stop_words.split(' ') # 스탑워드를 리스트에 넣는다.\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = [] \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
        "# result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print(word_tokens) \n",
        "print(result)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
            "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bCecz_4OMk8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}